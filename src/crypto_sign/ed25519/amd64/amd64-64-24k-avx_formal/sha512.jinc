require "shared_const.jinc"
require "sha512_globals.jinc"

u256[4] _0_128 = {
	(4u64)[0x1f1e1d1c1b1a1918, 0x1716151413121110, 0x0f0e0d0c0b0a0908, 0x0706050403020100],
	(4u64)[0x3f3e3d3c3b3a3938, 0x3736353433323130, 0x2f2e2d2c2b2a2928, 0x2726252423222120],
	(4u64)[0x5f5e5d5c5b5a5958, 0x5756555453525150, 0x4f4e4d4c4b4a4948, 0x4746454443424140],
	(4u64)[0x7f7e7d7c7b7a7978, 0x7776757473727170, 0x6f6e6d6c6b6a6968, 0x6766656463626160]
};

u256 x80 = (4u64)[0x8080808080808080, 0x8080808080808080, 0x8080808080808080, 0x8080808080808080];

inline fn __initH_ref() -> stack u64[8]
{
  inline int i;
  stack u64[8] H;
  reg ptr u64[8] Hp;
  reg u64 v;

  Hp = SHA512_H;

  for i=0 to 8
  { v = Hp[i];
    H[i] = v; }

  return H;
}

inline fn __load_H_ref(stack u64[8] H) -> reg u64, reg u64, reg u64, reg u64,
                                            reg u64, reg u64, reg u64, reg u64
{
  reg u64 a b c d e f g h;

  a = H[0];
  b = H[1];
  c = H[2];
  d = H[3];
  e = H[4];
  f = H[5];
  g = H[6];
  h = H[7];

  return a, b, c, d, e, f, g, h;
}

inline fn __store_H_ref(reg u64 a b c d e f g h) -> stack u64[8]
{
  stack u64[8] H;
  
  H[0] = a;
  H[1] = b;
  H[2] = c;
  H[3] = d;
  H[4] = e;
  H[5] = f;
  H[6] = g;
  H[7] = h;

  return H;
}

inline fn __ROTR_ref(reg u64 x, inline int c) -> reg u64
{
  reg u64 r;
  
  // Revisit, normal ror seems faster
  r = #RORX_64(x, c);
  return r;
}

//(x & y) ^ (!x & z)
inline fn __CH_ref(reg u64 x y z) -> reg u64
{
  reg u64 r s;

  r  =  x;
  r &=  y;
  s  =  !x & z;
  r ^=  s;

  return r;
}

//(x & y) ^ (x & z) ^ (y & z)
inline fn __MAJ_ref(reg u64 x y z) -> reg u64
{
  reg u64 r s;

  // Maybe keeping track of x & y allows reusing them next round
  r  = y;
  r ^= z;
  r &= x;
  
  s  = y;
  s &= z;
  r ^= s;

  return r;
}

// (x >>> 28) ^ (x >>> 34) ^ (x >>> 39)
inline fn __BSIG0_ref(reg u64 x) -> reg u64
{
  reg u64 r s;

  r  = __ROTR_ref(x, 28);
  s  = __ROTR_ref(x, 34);
  r ^= s;
  s  = __ROTR_ref(x, 39);
  r ^= s;

  return r;
}

// (x >>> 14) ^ (x >>> 18) ^ (x >>> 41)
inline fn __BSIG1_ref(reg u64 x) -> reg u64
{
  reg u64 r s;

  r  = __ROTR_ref(x, 14);
  s  = __ROTR_ref(x, 18);
  r ^= s;
  s  = __ROTR_ref(x, 41);
  r ^= s;

  return r;
}

// (x >>> 1) ^ (x >>> 8) ^ (x >> 7)
inline fn __SSIG0_avx(reg u256 X) -> reg u256
{
  reg u256 Xleft63 Xright1 Xsigma0;
  reg u256 Xleft56 Xright8;
  reg u256 Xright7;

  Xright1 = #VPSRL_4u64(X, 1);
  Xleft63 = #VPSLL_4u64(X, 63);
  Xsigma0 = Xright1 ^ Xleft63;
  
  Xright8 = #VPSRL_4u64(X, 8);
  Xsigma0 ^= Xright8;
  Xleft56 = #VPSLL_4u64(X, 56);
  Xsigma0 ^= Xleft56;
  
  Xright7 = #VPSRL_4u64(X, 7);
  Xsigma0 ^= Xright7;

  return Xsigma0;
}

inline fn __SSIG1_avx(reg u128 W) -> reg u256
{
  reg u128 Wleft45 Wright19 Wsigma1;
  reg u128 Wleft3 Wright61;
  reg u128 Wright6;
  reg u256 W_256;
  
  Wright19 = #VPSRL_2u64(W, 19);
  Wleft45 = #VPSLL_2u64(W, 45);
  Wsigma1 = Wleft45 ^ Wright19;
  
  Wright61 = #VPSRL_2u64(W, 61);
  Wsigma1 ^= Wright61;
  Wleft3 = #VPSLL_2u64(W, 3);
  Wsigma1 ^= Wleft3;
  
  Wright6 = #VPSRL_2u64(W, 6);
  
  W_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
  
  return W_256;
}

// Wt = SSIG1(W(t-2)) + W(t-7) + SSIG0(t-15) + W(t-16)
inline fn __Wt_part_avx(stack u64[20] W, reg ptr u64[80] Kp, reg u64 tr, inline int i) -> stack u64[20], reg u256
{
	reg u128 W_128 X_128;
	reg u256 D_256 Wsigma1_256 X0 X1;
	
	X0 = W.[u256 32*i];
	X1 = W.[u256 32*i + 8];
	X1 = __SSIG0_avx(X1);
	X0 = #VPADD_4u64(X0, X1);
	X0 = #VPADD_4u64(X0, W.[u256 (72 + 32*i) % 128]);
	
	W_128 = W.[u128 (112 + 32*i) % 128];
	Wsigma1_256 = __SSIG1_avx(W_128);
	X0 = #VPADD_4u64(X0, Wsigma1_256);
	
	X_128 = (128u) X0;
	Wsigma1_256 = __SSIG1_avx(X_128);
	Wsigma1_256 = #VPERMQ(Wsigma1_256, 0x4e);
	X0 = #VPADD_4u64(X0, Wsigma1_256);
	W[u256 i] = X0;
	
	D_256 = #VPADD_4u64(X0, Kp.[u256 (int)8*tr + 32*i]);
	
  return W, D_256;
}

// inline fn __inner_sha(reg u64 a b c d e f g h, reg ptr u64[80] Kp, reg u256[4] m) -> 
// reg u64, reg u64, reg u64, reg u64, reg u64, reg u64, reg u64, reg u64
// inline fn __inner_sha(stack u64[8] H, reg ptr u64[80] Kp, reg u256[4] m) -> stack u64[8]
inline fn __inner_sha(reg u64[8] H_r, reg ptr u64[80] Kp, reg u256[4] m) -> reg u64[8]
{
	reg u64 a b c d e f g h;
	reg u64 aAndb cAndd eAndf gAndh;
	reg u64 ch r18 r34 r39 r41;
	reg u64 aSigma0 bSigma0 cSigma0 dSigma0 eSigma0 fSigma0 gSigma0 hSigma0;
	reg u64 aSigma1 bSigma1 cSigma1 dSigma1 eSigma1 fSigma1 gSigma1 hSigma1;
	reg u64[8] maj;
	stack u64[8] H;
	stack u64[20] W;
	stack u64[16] wc;
	reg u128 Wleft3 Wleft45 Wright6 Wright19 Wright61 Wsigma1 X_128;
	reg u128[16] W_128;
	reg u256 bswap_256;
	reg u256 Wsigma1_256 Xleft56 Xleft63 Xright1 Xright7 Xright8 Xsigma0;
	reg u256[16] D X;
	inline int i;
	
	// a, b, c, d, e, f, g, h = __load_H_ref(H);
	a = H_r[0];
	b = H_r[1];
	c = H_r[2];
	d = H_r[3];
	e = H_r[4];
	f = H_r[5];
	g = H_r[6];
	h = H_r[7];
	
  bswap_256 = bswap_indices;
  
  X[0] = m[0];
  X[0] = #VPSHUFB_256(X[0], bswap_256);
  
  ch = g;
	eSigma1 = #RORX_64(e, 14);
	ch ^= f;
	
	D[0] = #VPADD_4u64(X[0], Kp[u256 0]);
	
	r18 = #RORX_64(e, 18);
	eSigma1 ^= r18;
	ch &= e;
	r41 = #RORX_64(e, 41);
	eSigma1 ^= r41;
	aSigma0 = #RORX_64(a, 28);
	ch ^= g;
	r34 = #RORX_64(a, 34);
	r39 = #RORX_64(a, 39);
	H[u64 7] = h;
	
	aSigma0 ^= r34;
	h += ch;
	maj[6] = b;
	maj[6] ^= a;
	aSigma0 ^= r39;
	aAndb = b;
	aAndb &= a;
	h += eSigma1;
	maj[7] = c;
	wc[u256 0] = #VMOVDQA_256(D[0]);
	h += wc[u64 0];
	maj[7] &= maj[6];
	H[u64 3] = d;
	
	d += h;
	h += aSigma0;
	maj[7] ^= aAndb;
	
	// Start of round 2 functions
	
	ch = f;
	dSigma1 = #RORX_64(d, 14);
	ch ^= e;
	
	X[4] = m[1];
	X[4] = #VPSHUFB_256(X[4], bswap_256);
	h += maj[7];
	r18 = #RORX_64(d, 18);
	D[4] = #VPADD_4u64(X[4], Kp[u256 1]);
	ch &= d;
	dSigma1 ^= r18;
	maj[6] &= h;
	
	H[6] = g;
	r41 = #RORX_64(d, 41);
	ch ^= f;
	maj[6] ^= aAndb;
	
	hSigma0 = #RORX_64(h, 28);
	H[5] = f;
	g += ch;
	dSigma1 ^= r41;
	r34 = #RORX_64(h, 34);
	H[2] = c;
	f += wc[u64 2];
	r39 = #RORX_64(h, 39);
	hSigma0 ^= r34;
	
	// Start of round 3 functions
	ch = e;
	ch ^= d;
	
	g += wc[u64 1];
	g += dSigma1;
	
	W[u256 0] = #VMOVDQA_256(X[0]);
	
	hSigma0 ^= r39;
	c += g;
	g += maj[6];
	H[4] = e;
	
	cSigma1 = #RORX_64(c, 14);
	g += hSigma0;
	ch &= c;
	r18 = #RORX_64(c, 18);
	W[u256 1] = #VMOVDQA_256(X[4]);
	cSigma1 ^= r18;
	ch ^= e;
	r41 = #RORX_64(c, 41);
	maj[4] = h;
	maj[4] ^= g;
	gSigma0 = #RORX_64(g, 28);
	wc[u256 1] = #VMOVDQA_256(D[4]);
	cSigma1 ^= r41;
	f += ch;
	r34 = #RORX_64(g, 34);
	gSigma0 ^= r34;
	r39 = #RORX_64(g, 39);
	gAndh = h;
	gAndh &= g;
	gSigma0 ^= r39;
	maj[5] = a;
	H[1] = b;
	f += cSigma1;
	e += wc[u64 3];
	maj[5] &= maj[4];
	b += f;
	f += gSigma0;
	maj[5] ^= gAndh;
	
	// Start of round 4 functions
	ch = d;
	bSigma1 = #RORX_64(b, 14);
	ch ^= c;
	f += maj[5];
	ch &= b;
	r18 = #RORX_64(b, 18);
	H[0] = a;
	bSigma1 ^= r18;
	maj[4] &= f;
	ch ^= d;
	r41 = #RORX_64(b, 41);
	X[8] = m[2];
	bSigma1 ^= r41;
	fSigma0 = #RORX_64(f, 28);
	maj[4] ^= gAndh;
	e += ch;
	r34 = #RORX_64(f, 34);
	e += bSigma1;
	fSigma0 ^= r34;
	d += wc[u64 4];
	X[12] = m[3];
	a += e;
	r39 = #RORX_64(f, 39);
	e += maj[4];
	fSigma0 ^= r39;
	aSigma1 = #RORX_64(a, 14);
	e += fSigma0;
	
	// Start of round 5 functions
	ch = c;
	r18 = #RORX_64(a, 18);
	ch ^= b;
	X[8] = #VPSHUFB_256(X[8], bswap_256);
	aSigma1 ^= r18;
	ch &= a;
	r41 = #RORX_64(a, 41);
	eSigma0 = #RORX_64(e, 28);
	D[8] = #VPADD_4u64(X[8], Kp[u256 2]);
	aSigma1 ^= r41;
	ch ^= c;
	W[u256 2] = #VMOVDQA_256(X[8]);
	d += ch;
	r34 = #RORX_64(e, 34);
	r39 = #RORX_64(e, 39);
	maj[2] = f;
	maj[2] ^= e;
	wc[u256 2] = #VMOVDQA_256(D[8]);
	eSigma0 ^= r34;
	d += aSigma1;
	eAndf = f;
	eAndf &= e;
	c += wc[u64 5];
	eSigma0 ^= r39;
	maj[3] = g;
	maj[3] &= maj[2];
	h += d;
	d += eSigma0;
	X[12] = #VPSHUFB_256(X[12], bswap_256);
	
	// Start of round 6 functions
	ch = b;
	maj[3] ^= eAndf;
	ch ^= a;
	d += maj[3];
	hSigma1 = #RORX_64(h, 14);
	D[12] = #VPADD_4u64(X[12], Kp[u256 3]);
	ch &= h;
	r18 = #RORX_64(h, 18);
	hSigma1 ^= r18;
	maj[2] &= d;
	ch ^= b;
	r41 = #RORX_64(h, 41);
	W[u256 3] = #VMOVDQA_256(X[12]);
	hSigma1 ^= r41;
	maj[2] ^= eAndf;
	dSigma0 = #RORX_64(d, 28);
	wc[u256 3] = #VMOVDQA_256(D[12]);
	c += ch;
	c += hSigma1;
	b += wc[u64 6];
	
	// Start of round 7 functions
	ch = a;
	r34 = #RORX_64(d, 34);
	ch ^= h;
	dSigma0 ^= r34;
	g += c;
	r39 = #RORX_64(d, 39);
	c += maj[2];
	dSigma0 ^= r39;
	gSigma1 = #RORX_64(g, 14);
	r18 = #RORX_64(g, 18);
	r41 = #RORX_64(g, 41);
	ch &= g;
	c += dSigma0;
	gSigma1 ^= r18;
	cSigma0 = #RORX_64(c, 28);
	gSigma1 ^= r41;
	ch ^= a;
	r34 = #RORX_64(c, 34);
	maj[0] = d;
	maj[0] ^= c;
	cSigma0 ^= r34;
	b += ch;
	r39 = #RORX_64(c, 39);
	cAndd = d;
	cAndd &= c;
	cSigma0 ^= r39;
	b += gSigma1;
	maj[1] = e;
	maj[1] &= maj[0];
	a += wc[u64 7];
	f += b;
	b += cSigma0;
	
	// Start of round 8 functions
	ch = h;
	maj[1] ^= cAndd;
	ch ^= g;
	fSigma1 = #RORX_64(f, 14);
	b += maj[1];
	ch &= f;
	r18 = #RORX_64(f, 18);
	fSigma1 ^= r18;
	maj[0] &= b;
	ch ^= h;
	r41 = #RORX_64(f, 41);
	fSigma1 ^= r41;
	maj[0] ^= cAndd;
	bSigma0 = #RORX_64(b, 28);
	a += ch;
	a += fSigma1;
	r34 = #RORX_64(b, 34);
	bSigma0 ^= r34;
	e += a;
	a += maj[0];
	r39 = #RORX_64(b, 39);
	bSigma0 ^= r39;
	a += bSigma0;
	
	
	for i=0 to 4
	{
		// Round 1 functions
		X[1] = W.[u256 8];
		Xright1 = #VPSRL_4u64(X[1], 1);
		eSigma1 = #RORX_64(e, 14);
		h += wc[u64 8];
		Xleft63 = #VPSLL_4u64(X[1], 63);
		ch = g;
		ch ^= f;
		r18 = #RORX_64(e, 18);
		ch &= e;
		maj[6] = b;
		maj[6] ^= a;
		W_128[14] = W.[u128 112];
		r41 = #RORX_64(e, 41);
		eSigma1 ^= r18;
		ch ^= g;
		Xsigma0 = Xright1 ^ Xleft63;
		Xright8 = #VPSRL_4u64(X[1], 8);
		eSigma1 ^= r41;
		h += ch;
		aSigma0 = #RORX_64(a, 28);
		r34 = #RORX_64(a, 34);
		Xsigma0 ^= Xright8;
		h += eSigma1;
		maj[7] = c;
		maj[7] &= maj[6];
		aSigma0 ^= r34;
		Wright19 = #VPSRL_2u64(W_128[14], 19);
		r39 = #RORX_64(a, 39);
		Xleft56 = #VPSLL_4u64(X[1], 56);
		d += h;
		aSigma0 ^= r39;
		
		g += wc[u64 9];
		aAndb  = b;
		aAndb &= a;
		Wleft45 = #VPSLL_2u64(W_128[14], 45);
		h += aSigma0;
		maj[7] ^= aAndb;
		
		// Start of round 2 functions
		ch = f;
		ch ^= e;
		Wright61 = #VPSRL_2u64(W_128[14], 61);
		dSigma1 = #RORX_64(d, 14);
		Xsigma0 ^= Xleft56;
		h += maj[7];
		r18 = #RORX_64(d, 18);
		Xright7 = #VPSRL_4u64(X[1], 7);
		Wsigma1 = Wleft45 ^ Wright19;
		ch &= d;
		dSigma1 ^= r18;
		r41 = #RORX_64(d, 41);
		maj[6] &= h;
		Xsigma0 ^= Xright7;
		Wsigma1 ^= Wright61;
		X[0] = #VPADD_4u64(X[0], Xsigma0);
		dSigma1 ^= r41;
		maj[6] ^= aAndb;
		Wleft3 = #VPSLL_2u64(W_128[14], 3);
		hSigma0 = #RORX_64(h, 28);
		ch ^= f;
		g += dSigma1;
		X[0] = #VPADD_4u64(X[0], W.[u256 72]);
		r34 = #RORX_64(h, 34);
		
		f += wc[u64 10];
		g += ch;
		hSigma0 ^= r34;
		r39 = #RORX_64(h, 39);
		c += g;
		Wsigma1 ^= Wleft3;
		Wright6 = #VPSRL_2u64(W_128[14], 6);
		hSigma0 ^= r39;
		g += maj[6];
		
		// Start of round 3 functions
		ch = e;
		ch ^= d;
		g += hSigma0;
		cSigma1 = #RORX_64(c, 14);
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		ch &= c;
		r18 = #RORX_64(c, 18);
		r41 = #RORX_64(c, 41);
		X[0] = #VPADD_4u64(X[0], Wsigma1_256);
		ch ^= e;
		cSigma1 ^= r18;
		
		X_128 = (128u) X[0];
		Wright19 = #VPSRL_2u64(X_128, 19);
		gSigma0 = #RORX_64(g, 28);
		f += ch;
		Wleft45 = #VPSLL_2u64(X_128, 45);
		r34 = #RORX_64(g, 34);
		cSigma1 ^= r41;
		maj[4] = h;
		X[5] = W.[u256 40];
		maj[4] ^= g;
		Wright61 = #VPSRL_2u64(X_128, 61);
		Wsigma1 = Wright19 ^ Wleft45;
		gSigma0 ^= r34;
		Wsigma1 ^= Wright61;
		r39 = #RORX_64(g, 39);
		Wleft3 = #VPSLL_2u64(X_128, 3);
		gSigma0 ^= r39;
		f += cSigma1;
		Wright6 = #VPSRL_2u64(X_128, 6);
		Wsigma1 ^= Wleft3;
		b += f;
		gAndh = h;
		gAndh &= g;
		bSigma1 = #RORX_64(b, 14);
		f += gSigma0;
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		maj[5] = a;
		maj[5] &= maj[4];
		Wsigma1_256 = #VPERMQ(Wsigma1_256, 0x4e);
		maj[5] ^= gAndh;
		
		// Start of round 4 functions
		ch = d;
		Xright1 = #VPSRL_4u64(X[5], 1);
		ch ^= c;
		f += maj[5];
		r18 = #RORX_64(b, 18);
		ch &= b;
		maj[4] &= f;
		ch ^= d;
		e += wc[u64 11];
		bSigma1 ^= r18;
		r41 = #RORX_64(b, 41);
		X[0] = #VPADD_4u64(X[0], Wsigma1_256);
		e += ch;
		maj[4] ^= gAndh;
		fSigma0 = #RORX_64(f, 28);
		D[0] = #VPADD_4u64(X[0], Kp[u256 4*i+4]);
		bSigma1 ^= r41;
		r34 = #RORX_64(f, 34);
		W[u256 4] = #VMOVDQA_256(X[0]);
		e += bSigma1;
		fSigma0 ^= r34;
		r39 = #RORX_64(f, 39);
		d += wc[u64 12];
		W[u256 0] = #VMOVDQA_256(X[0]);
		a += e;
		e += maj[4];
		fSigma0 ^= r39;
		wc[u256 0] = #VMOVDQA_256(D[0]);
		W_128[2] = W.[u128 16];
		e += fSigma0;
		aSigma1 = #RORX_64(a, 14);
		
		
		// Start of round 5 functions
		ch = c;
		ch ^= b;
		Xleft63 = #VPSLL_4u64(X[5], 63);
		r18 = #RORX_64(a, 18);
		maj[2] = f;
		ch &= a;
		maj[2] ^= e;
		r41 = #RORX_64(a, 41);
		Xsigma0 = Xright1 ^ Xleft63;
		aSigma1 ^= r18;
		ch ^= c;
		Xright8 = #VPSRL_4u64(X[5], 8);
		eSigma0 = #RORX_64(e, 28);
		aSigma1 ^= r41;
		d += ch;
		r34 = #RORX_64(e, 34);
		Xsigma0 ^= Xright8;
		d += aSigma1;
		maj[3] = g;
		maj[3] &= maj[2];
		eSigma0 ^= r34;
		r39 = #RORX_64(e, 39);
		Wright19 = #VPSRL_2u64(W_128[2], 19);
		Xleft56 = #VPSLL_4u64(X[5], 56);
		h += d;
		eSigma0 ^= r39;
		c += wc[u64 13];
		eAndf = f;
		eAndf &= e;
		Wleft45 = #VPSLL_2u64(W_128[2], 45);
		d += eSigma0;
		maj[3] ^= eAndf;
		
		// Start of round 6 functions
		ch = b;
		ch ^= a;
		Wright61 = #VPSRL_2u64(W_128[2], 61);
		hSigma1 = #RORX_64(h, 14);
		Xsigma0 ^= Xleft56;
		d += maj[3];
		Xright7 = #VPSRL_4u64(X[5], 7);
		r18 = #RORX_64(h, 18);
		Wsigma1 = Wright19 ^ Wleft45;
		ch &= h;
		hSigma1 ^= r18;
		Xsigma0 ^= Xright7;

		r41 = #RORX_64(h, 41);
		maj[2] &= d;
		Wsigma1 ^= Wright61;
		X[4] = #VPADD_4u64(X[4], Xsigma0);
		hSigma1 ^= r41;
		maj[2] ^= eAndf;
		Wleft3 = #VPSLL_2u64(W_128[2], 3);
		dSigma0 = #RORX_64(d, 28);
		ch ^= b;
		c += hSigma1;
		r34 = #RORX_64(d, 34);
		X[4] = #VPADD_4u64(X[4], W.[u256 104]);
		
		b += wc[u64 14];
		c += ch;
		dSigma0 ^= r34;
		r39 = #RORX_64(d, 39);
		g += c;
		Wsigma1 ^= Wleft3;
		Wright6 = #VPSRL_2u64(W_128[2], 6);
		dSigma0 ^= r39;
		c += maj[2];
		
		// Start of round 7 functions
		ch = a;
		ch ^= h;
		c += dSigma0;
		gSigma1 = #RORX_64(g, 14);
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		ch &= g;
		r18 = #RORX_64(g, 18);
		r41 = #RORX_64(g, 41);
		X[4] = #VPADD_4u64(X[4], Wsigma1_256);
		ch ^= a;
		gSigma1 ^= r18;
		
		X_128 = (128u) X[4];
		Wright19 = #VPSRL_2u64(X_128, 19);
		b += ch;
		cSigma0 = #RORX_64(c, 28);
		Wleft45 = #VPSLL_2u64(X_128, 45);
		gSigma1 ^= r41;
		r34 = #RORX_64(c, 34);
		maj[0] = d;
		maj[0] ^= c;
		Wright61 = #VPSRL_2u64(X_128, 61);
		X[9] = W.[u256 72];
		cSigma0 ^= r34;
		Wsigma1 = Wright19 ^ Wleft45;
		r39 = #RORX_64(c, 39);
		Wleft3 = #VPSLL_2u64(X_128, 3);
		Wsigma1 ^= Wright61;
		Wright6 = #VPSRL_2u64(X_128, 6);
		cSigma0 ^= r39;
		b += gSigma1;
		Wsigma1 ^= Wleft3;
		f += b;
		cAndd = d;
		fSigma1 = #RORX_64(f, 14);
		cAndd &= c;
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		b += cSigma0;
		maj[1] = e;
		maj[1] &= maj[0];
		Wsigma1_256 = #VPERMQ(Wsigma1_256, 0x4e);
		maj[1] ^= cAndd;
		
		// Start of round 8 functions
		ch = h;
		Xright1 = #VPSRL_4u64(X[9], 1);
		ch ^= g;
		b += maj[1];
		ch &= f;
		r18 = #RORX_64(f, 18);
		maj[0] &= b;
		
		a += wc[u64 15];
		ch ^= h;
		fSigma1 ^= r18;
		X[4] = #VPADD_4u64(X[4], Wsigma1_256);
		r41 = #RORX_64(f, 41);
		W[u256 1] = #VMOVDQA_256(X[4]);
		a += ch;
		maj[0] ^= cAndd;
		bSigma0 = #RORX_64(b, 28);
		D[4] = #VPADD_4u64(X[4], Kp[u256 4*i+5]);
		wc[u256 1] = #VMOVDQA_256(D[4]);
		fSigma1 ^= r41;
		r34 = #RORX_64(b, 34);
		a += fSigma1;
		bSigma0 ^= r34;
		h += wc[0];
		r39 = #RORX_64(b, 39);
		e += a;
		a += maj[0];
		bSigma0 ^= r39;
		eSigma1 = #RORX_64(e, 14);
		W_128[6] = W.[u128 48];
		a += bSigma0;
		
		// Start of round 1 functions
		ch = g;
		ch ^= f;
		r18 = #RORX_64(e, 18);
		Xleft63 = #VPSLL_4u64(X[9], 63);
		ch &= e;
		maj[6] = b;
		maj[6] ^= a;
		r41 = #RORX_64(e, 41);
		Xsigma0 = Xright1 ^ Xleft63;
		eSigma1 ^= r18;
		Xright8 = #VPSRL_4u64(X[9], 8);
		ch ^= g;
		eSigma1 ^= r41;
		aSigma0 = #RORX_64(a, 28);
		h += ch;
		Xsigma0 ^= Xright8;
		h += eSigma1;
		r34 = #RORX_64(a, 34);
		maj[7] = c;
		maj[7] &= maj[6];
		aSigma0 ^= r34;
		Wright19 = #VPSRL_2u64(W_128[6], 19);
		r39 = #RORX_64(a, 39);
		d += h;
		aSigma0 ^= r39;
		Xleft56 = #VPSLL_4u64(X[9], 56);
		g += wc[u64 1];
		aAndb  = b;
		aAndb &= a;
		h += aSigma0;
		
		// Start of round 2 functions
		ch = f;
		Wleft45 = #VPSLL_2u64(W_128[6], 45);
		maj[7] ^= aAndb;
		ch ^= e;
		Wright61 = #VPSRL_2u64(W_128[6], 61);
		dSigma1 = #RORX_64(d, 14);
		h += maj[7];
		Xsigma0 ^= Xleft56;
		Xright7 = #VPSRL_4u64(X[9], 7);
		r18 = #RORX_64(d, 18);
		Wsigma1 = Wleft45 ^ Wright19;
		ch &= d;
		dSigma1 ^= r18;
		r41 = #RORX_64(d, 41);
		Xsigma0 ^= Xright7;
		maj[6] &= h;
		Wsigma1 ^= Wright61;
		X[8] = #VPADD_4u64(X[8], Xsigma0);
		dSigma1 ^= r41;
		maj[6] ^= aAndb;
		Wleft3 = #VPSLL_2u64(W_128[6], 3);
		hSigma0 = #RORX_64(h, 28);
		ch ^= f;
		g += dSigma1;
		X[8] = #VPADD_4u64(X[8], W.[u256 8]);
		r34 = #RORX_64(h, 34);
		
		f += wc[u64 2];
		g += ch;
		hSigma0 ^= r34;
		Wsigma1 ^= Wleft3;
		r39 = #RORX_64(h, 39);
		c += g;
		hSigma0 ^= r39;
		
		// Start of round 3 functions
		ch = e;
		Wright6 = #VPSRL_2u64(W_128[6], 6);
		g += maj[6];
		ch ^= d;
		g += hSigma0;
		cSigma1 = #RORX_64(c, 14);
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		ch &= c;
		r18 = #RORX_64(c, 18);
		r41 = #RORX_64(c, 41);
		X[8] = #VPADD_4u64(X[8], Wsigma1_256);
		
		X_128 = (128u) X[8];
		Wright19 = #VPSRL_2u64(X_128, 19);
		ch ^= e;
		cSigma1 ^= r18;
		gSigma0 = #RORX_64(g, 28);
		f += ch;
		Wleft45 = #VPSLL_2u64(X_128, 45);
		r34 = #RORX_64(g, 34);
		cSigma1 ^= r41;
		maj[4] = h;
		Wright61 = #VPSRL_2u64(X_128, 61);
		maj[4] ^= g;
		gSigma0 ^= r34;
		Wsigma1 = Wright19 ^ Wleft45;
		r39 = #RORX_64(g, 39);
		Wsigma1 ^= Wright61;
		Wleft3 = #VPSLL_2u64(X_128, 3);
		gSigma0 ^= r39;
		f += cSigma1;
		b += f;
		Wright6 = #VPSRL_2u64(X_128, 6);
		Wsigma1 ^= Wleft3;
		gAndh = h;
		gAndh &= g;
		bSigma1 = #RORX_64(b, 14);
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		f += gSigma0;
		maj[5] = a;
		maj[5] &= maj[4];
		Wsigma1_256 = #VPERMQ(Wsigma1_256, 0x4e);
		maj[5] ^= gAndh;
		
		// Start of round 4 functions
		ch = d;
		ch ^= c;
		X[13] = W.[u256 104];
		Xright1 = #VPSRL_4u64(X[13], 1);
		f += maj[5];
		r18 = #RORX_64(b, 18);
		ch &= b;
		maj[4] &= f;
		ch ^= d;
		e += wc[u64 3];
		bSigma1 ^= r18;
		r41 = #RORX_64(b, 41);
		X[8] = #VPADD_4u64(X[8], Wsigma1_256);
		W[u256 2] = #VMOVDQA_256(X[8]);
		e += ch;
		maj[4] ^= gAndh;
		fSigma0 = #RORX_64(f, 28);
		D[8] = #VPADD_4u64(X[8], Kp[u256 4*i+6]);
		wc[u256 2] = #VMOVDQA_256(D[8]);
		r34 = #RORX_64(f, 34);
		bSigma1 ^= r41;
		e += bSigma1;
		fSigma0 ^= r34;
		r39 = #RORX_64(f, 39);
		d += wc[u64 4];
		a += e;
		e += maj[4];
		W_128[10] = W.[u128 80];
		fSigma0 ^= r39;
		aSigma1 = #RORX_64(a, 14);
		e += fSigma0;
		
		// Start of round 5 functions
		ch = c;
		ch ^= b;
		ch &= a;
		Xleft63 = #VPSLL_4u64(X[13], 63);
		r18 = #RORX_64(a, 18);
		maj[2] = f;
		maj[2] ^= e;
		Xsigma0 = Xright1 ^ Xleft63;
		r41 = #RORX_64(a, 41);
		aSigma1 ^= r18;
		ch ^= c;
		Xright8 = #VPSRL_4u64(X[13], 8);
		eSigma0 = #RORX_64(e, 28);
		aSigma1 ^= r41;
		d += ch;
		r34 = #RORX_64(e, 34);
		Xsigma0 ^= Xright8;
		d += aSigma1;
		maj[3] = g;
		maj[3] &= maj[2];
		Wright19 = #VPSRL_2u64(W_128[10], 19);
		eSigma0 ^= r34;
		h += d;
		r39 = #RORX_64(e, 39);
		Xleft56 = #VPSLL_4u64(X[13], 56);
		eSigma0 ^= r39;
		c += wc[u64 5];
		eAndf = f;
		Wleft45 = #VPSLL_2u64(W_128[10], 45);
		eAndf &= e;
		d += eSigma0;
		maj[3] ^= eAndf;
		
		// Start of round 6 functions
		ch = b;
		Wright61 = #VPSRL_2u64(W_128[10], 61);
		ch ^= a;
		Xsigma0 ^= Xleft56;
		hSigma1 = #RORX_64(h, 14);
		Xright7 = #VPSRL_4u64(X[13], 7);
		d += maj[3];
		r18 = #RORX_64(h, 18);
		Wsigma1 = Wright19 ^ Wleft45;
		ch &= h;
		hSigma1 ^= r18;
		r41 = #RORX_64(h, 41);
		Xsigma0 ^= Xright7;

		maj[2] &= d;
		Wsigma1 ^= Wright61;
		X[12] = #VPADD_4u64(X[12], Xsigma0);
		hSigma1 ^= r41;
		maj[2] ^= eAndf;
		Wleft3 = #VPSLL_2u64(W_128[10], 3);
		dSigma0 = #RORX_64(d, 28);
		ch ^= b;
		c += hSigma1;
		r34 = #RORX_64(d, 34);
		X[12] = #VPADD_4u64(X[12], W.[u256 40]);
		
		b += wc[u64 6];
		c += ch;
		Wsigma1 ^= Wleft3;
		dSigma0 ^= r34;
		r39 = #RORX_64(d, 39);
		Wright6 = #VPSRL_2u64(W_128[10], 6);
		g += c;
		dSigma0 ^= r39;
		
		// Start of round 7 functions
		ch = a;
		c += maj[2];
		ch ^= h;
		c += dSigma0;
		gSigma1 = #RORX_64(g, 14);
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		ch &= g;
		r18 = #RORX_64(g, 18);
		ch ^= a;
		X[12] = #VPADD_4u64(X[12], Wsigma1_256);
		r41 = #RORX_64(g, 41);
		
		X_128 = (128u) X[12];
		Wright19 = #VPSRL_2u64(X_128, 19);
		gSigma1 ^= r18;
		b += ch;
		cSigma0 = #RORX_64(c, 28);
		Wleft45 = #VPSLL_2u64(X_128, 45);
		gSigma1 ^= r41;
		r34 = #RORX_64(c, 34);
		maj[0] = d;
		maj[0] ^= c;
		cSigma0 ^= r34;
		Wright61 = #VPSRL_2u64(X_128, 61);
		r39 = #RORX_64(c, 39);
		Wsigma1 = Wright19 ^ Wleft45;
		
		Wleft3 = #VPSLL_2u64(X_128, 3);
		Wsigma1 ^= Wright61;
		cSigma0 ^= r39;
		b += gSigma1;
		Wright6 = #VPSRL_2u64(X_128, 6);
		f += b;
		Wsigma1 ^= Wleft3;
		cAndd = d;
		cAndd &= c;
		fSigma1 = #RORX_64(f, 14);
		b += cSigma0;
		Wsigma1_256 = (256u)#VPXOR_128(Wsigma1, Wright6);
		maj[1] = e;
		maj[1] &= maj[0];
		Wsigma1_256 = #VPERMQ(Wsigma1_256, 0x4e);
		maj[1] ^= cAndd;
		
		// Start of round 8 functions
		ch = h;
		ch ^= g;
		b += maj[1];
		ch &= f;
		r18 = #RORX_64(f, 18);
		maj[0] &= b;
		a += wc[u64 7];
		ch ^= h;
		fSigma1 ^= r18;
		r41 = #RORX_64(f, 41);
		X[12] = #VPADD_4u64(X[12], Wsigma1_256);
		a += ch;
		W[u256 3] = #VMOVDQA_256(X[12]);
		maj[0] ^= cAndd;
		bSigma0 = #RORX_64(b, 28);
		D[12] = #VPADD_4u64(X[12], Kp[u256 4*i+7]);
		wc[u256 3] = #VMOVDQA_256(D[12]);
		fSigma1 ^= r41;
		r34 = #RORX_64(b, 34);
		a += fSigma1;
		bSigma0 ^= r34;
		r39 = #RORX_64(b, 39);
		e += a;
		a += maj[0];
		bSigma0 ^= r39;
		a += bSigma0;
	}
	
	
	// Start of round 1 functions
	h += wc[u64 8];
  eSigma1 = #RORX_64(e, 14);
  ch = g;
	r18 = #RORX_64(e, 18);
	ch ^= f;
	r41 = #RORX_64(e, 41);
	eSigma1 ^= r18;
	ch &= e;
	aSigma0 = #RORX_64(a, 28);
	eSigma1 ^= r41;
	
	ch ^= g;
	r34 = #RORX_64(a, 34);
	maj[6] = b;
	maj[6] ^= a;
	aSigma0 ^= r34;
	h += ch;
	aAndb = b;
	r39 = #RORX_64(a, 39);
	aAndb &= a;
	aSigma0 ^= r39;
	h += eSigma1;
	maj[7] = c;
	g += wc[u64 9];
	maj[7] &= maj[6];
	d += h;
	h += aSigma0;	
	
	// Start of round 2 functions
	ch = f;
	maj[7] ^= aAndb;
	ch ^= e;
	dSigma1 = #RORX_64(d, 14);
	h += maj[7];
	ch &= d;
	r18 = #RORX_64(d, 18);
	dSigma1 ^= r18;
	maj[6] &= h;
	ch ^= f;
	r41 = #RORX_64(d, 41);
	dSigma1 ^= r41;
	hSigma0 = #RORX_64(h, 28);
	maj[6] ^= aAndb;
	g += ch;
	g += dSigma1;
	r34 = #RORX_64(h, 34);
	hSigma0 ^= r34;
	c += g;
	g += maj[6];
	r39 = #RORX_64(h, 39);
	hSigma0 ^= r39;
	g += hSigma0;
	f += wc[u64 10];
	cSigma1 = #RORX_64(c, 14);
	
	// Start of round 3 functions
	ch = e;
	r18 = #RORX_64(c, 18);
	ch ^= d;
	r41 = #RORX_64(c, 41);
	cSigma1 ^= r18;
	ch &= c;
	gSigma0 = #RORX_64(g, 28);
	cSigma1 ^= r41;
	ch ^= e;
	r34 = #RORX_64(g, 34);
	maj[4] = h;
	maj[4] ^= g;
	gSigma0 ^= r34;
	f += ch;
	gAndh = h;
	r39 = #RORX_64(g, 39);	
	gAndh &= g;
	gSigma0 ^= r39;
	f += cSigma1;
	maj[5] = a;
	e += wc[u64 11];
	maj[5] &= maj[4];
	b += f;
	f += gSigma0;
	
	// Start of round 4 functions
	ch = d;
	maj[5] ^= gAndh;
	ch ^= c;
	bSigma1 = #RORX_64(b, 14);
	f += maj[5];
	ch &= b;
	r18 = #RORX_64(b, 18);
	bSigma1 ^= r18;
	maj[4] &= f;
	ch ^= d;
	r41 = #RORX_64(b, 41);
	bSigma1 ^= r41;
	fSigma0 = #RORX_64(f, 28);
	maj[4] ^= gAndh;
	e += ch;
	e += bSigma1;
	r34 = #RORX_64(f, 34);
	fSigma0 ^= r34;
	a += e;
	e += maj[4];
	r39 = #RORX_64(f, 39);
	fSigma0 ^= r39;
	e += fSigma0;
	d += wc[u64 12];
	aSigma1 = #RORX_64(a, 14);
	
	// Start of round 5 functions
	ch = c;
	r18 = #RORX_64(a, 18);
	ch ^= b;
	r41 = #RORX_64(a, 41);
	aSigma1 ^= r18;
	ch &= a;
	eSigma0 = #RORX_64(e, 28);
	aSigma1 ^= r41;
	ch ^= c;
	r34 = #RORX_64(e, 34);
	maj[2] = f;
	maj[2] ^= e;
	eSigma0 ^= r34;
	d += ch;
	eAndf = f;
	r39 = #RORX_64(e, 39);
	eAndf &= e;
	eSigma0 ^= r39;
	d += aSigma1;
	maj[3] = g;
	c += wc[u64 13];
	maj[3] &= maj[2];
	h += d;
	d += eSigma0;
	
	// Start of round 6 functions
	ch = b;
	maj[3] ^= eAndf;
	ch ^= a;
	hSigma1 = #RORX_64(h, 14);
	d += maj[3];
	ch &= h;
	r18 = #RORX_64(h, 18);
	hSigma1 ^= r18;
	maj[2] &= d;
	ch ^= b;
	r41 = #RORX_64(h, 41);
	hSigma1 ^= r41;
	dSigma0 = #RORX_64(d, 28);
	maj[2] ^= eAndf;
	c += ch;
	c += hSigma1;
	r34 = #RORX_64(d, 34);
	dSigma0 ^= r34;
	g += c;
	c += maj[2];
	r39 = #RORX_64(d, 39);
	dSigma0 ^= r39;
	c += dSigma0;
	b += wc[u64 14];
	gSigma1 = #RORX_64(g, 14);
	
	// Start of round 7 functions
	ch = a;
	r18 = #RORX_64(g, 18);
	ch ^= h;
	r41 = #RORX_64(g, 41);
	gSigma1 ^= r18;
	ch &= g;
	cSigma0 = #RORX_64(c, 28);
	gSigma1 ^= r41;
	ch ^= a;
	r34 = #RORX_64(c, 34);
	maj[0] = d;
	maj[0] ^= c;
	cSigma0 ^= r34;
	b += ch;
	cAndd = d;
	r39 = #RORX_64(c, 39);
	cAndd &= c;
	cSigma0 ^= r39;
	b += gSigma1;
	maj[1] = e;
	a += wc[u64 15];
	maj[1] &= maj[0];
	f += b;
	b += cSigma0;
	
	// Start of round 8 functions
	ch = h;
	maj[1] ^= cAndd;
	ch ^= g;
	fSigma1 = #RORX_64(f, 14);
	b += maj[1];
	ch &= f;
	r18 = #RORX_64(f, 18);
	fSigma1 ^= r18;
	maj[0] &= b;
	ch ^= h;
	r41 = #RORX_64(f, 41);
	fSigma1 ^= r41;
	bSigma0 = #RORX_64(b, 28);
	maj[0] ^= cAndd;
	a += ch;
	a += fSigma1;
	r34 = #RORX_64(b, 34);
	bSigma0 ^= r34;
	e += a;
	a += maj[0];
	r39 = #RORX_64(b, 39);
	bSigma0 ^= r39;
	a += bSigma0;
	
	a += H[0];
	b += H[1];
	c += H[2];
	d += H[3];
	e += H[4];
	f += H[5];
	g += H[6];
	h += H[7];
	
  // H = __store_H_ref(a, b, c, d, e, f, g, h);
  
  // return H;
  H_r[0] = a;
  H_r[1] = b;
  H_r[2] = c;
  H_r[3] = d;
  H_r[4] = e;
  H_r[5] = f;
  H_r[6] = g;
  H_r[7] = h;
  
  return H_r;
}


inline fn _blocks_0_ref(reg u256[4] m, stack u64[8] H, #msf reg u64 msf) -> stack u64[8], #msf reg u64
{
  reg ptr u64[80] Kp;
  #mmx reg u64 msf_s;
  
  Kp = SHA512_K;
  
	msf_s = #mov_msf(msf);
  H = __inner_sha(H, Kp, m);
  msf = #mov_msf(msf_s);
  
  return H, msf;
}


#[sct="{ ptr: public, val: { n: d, s:secret } } * { ptr: public, val: { n: d, s:secret } } * secret * msf -> { ptr: public, val: { n: d, s:secret } } * { ptr: public, val: { n: d, s:secret } } * msf"]
inline fn _blocks_1_ref(reg ptr u64[8] H, reg ptr u64[32] sblocks, reg u64 nblocks, #msf reg u64 msf) -> reg ptr u64[8], reg ptr u64[32], #msf reg u64
{
  #public #spill_to_mmx reg u64 i;
  reg u256 v_256;
  reg u64[8] H_r;
  reg u256[4] m_buf;
  #mmx reg ptr u64[8] Hptr_s;
  stack ptr u64[32] sblocks_s;
  reg ptr u64[80] Kp;
  reg bool cond;
  inline int t;
  #mmx reg u64 msf_s;
  
  Kp = SHA512_K;
  
  /*v_256 = #VMOVDQA_256(H[u256 0]);
  Hs[u256 0] = #VMOVDQA_256(v_256);
  v_256 = #VMOVDQA_256(H[u256 1]);
  Hs[u256 1] = #VMOVDQA_256(v_256);*/
  for t=0 to 8
  { H_r[t] = H[t]; }
  Hptr_s = H;
  
  #declassify i = nblocks;
  i = #protect(i, msf);
  
  while{cond = (i != 0); }(cond)
  {
    msf = #update_msf(cond, msf);
    
    () = #spill(i);
    
    sblocks_s = sblocks;
		msf_s = #mov_msf(msf);
		
		for t=0 to 4
		{
			m_buf[t] = #VMOVDQA_256(sblocks[u256 t]);
		}
		
		// Hs = __inner_sha(Hs, Kp, m_buf);
		H_r = __inner_sha(H_r, Kp, m_buf);
		msf = #mov_msf(msf_s);
		sblocks = sblocks_s;
		
		for t=0 to 4
    { 
      // Shift sblocks to the left
      v_256 = #VMOVDQA_256(sblocks[u256 4+t]);
      sblocks[u256 t] = #VMOVDQA_256(v_256);
    }
    
    () = #unspill(i);
    i -= 1;
  }
  msf = #update_msf(!cond, msf);
	
	H = Hptr_s;
	/*v_256 = #VMOVDQA_256(Hs[u256 0]);
	H[u256 0] = #VMOVDQA_256(v_256);
	v_256 = #VMOVDQA_256(Hs[u256 1]);
	H[u256 1] = #VMOVDQA_256(v_256);*/
	for t=0 to 8
  { H[t] = H_r[t]; }
	
  return H, sblocks, msf;
}

#[sct="{ n: d, s:secret } * secret * secret * msf -> { n: d, s:secret } * secret * msf"]
inline fn __lastblocks_ref(stack u8[128] in, reg u64 inlen bits, #msf reg u64 msf) -> stack u64[32], reg u64, #msf reg u64
{
  stack u64[32] sblocks;
  inline int k;
  reg u64 nblocks nblocks2 v_64;
  reg u256 cmp_eq cmp_lt inlen_256 v_256 z_256;
  reg bool l;
	
	z_256 = #set0_256();
  // Zero-fill the sblocks array
  for k = 4 to 8 { sblocks[u256 k] = #VMOVDQA_256(z_256); }
  
  inlen_256 = (256u)#VMOV_64(inlen);
  inlen_256 = #VPBROADCAST_32u8(inlen_256);
  
	for k = 0 to 4
	{
		cmp_lt = #VPCMPGT_32u8(inlen_256, _0_128[k]);
		cmp_eq = #VPCMPEQ_32u8(inlen_256, _0_128[k]);
		v_256 = #VMOVDQA_256(in[u256 k]);
		v_256 = #VPBLENDVB_256(z_256, v_256, cmp_lt);
		v_256 = #VPBLENDVB_256(v_256, x80, cmp_eq);
		
		sblocks[u256 k] = #VMOVDQA_256(v_256);
	}

  // check if one or two blocks are needed
  ?{"<u" = l} = #CMP(inlen, 112);
  
  nblocks = 1;
  nblocks2 = 2;
  nblocks = #CMOVcc(l, nblocks, nblocks2);
	bits = #BSWAP_64(bits);
	
	v_64 = bits;
	v_64 = #CMOVcc(!l, sblocks[15], v_64);
	sblocks[15] = v_64;
	
	v_64 = bits;
	v_64 = #CMOVcc(l, sblocks[31], v_64);
	sblocks[31] = v_64;
	
  return sblocks, nblocks, msf;
}
