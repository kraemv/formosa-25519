require "shared_const.jinc"

from formosa25519 require "crypto_sign/ed25519/amd64/common/64/add4.jinc"
from formosa25519 require "crypto_sign/ed25519/amd64/common/64/add5.jinc"
from formosa25519 require "crypto_sign/ed25519/amd64/common/64/sub4.jinc"
from formosa25519 require "crypto_sign/ed25519/amd64/common/64/sub5.jinc"
from formosa25519 require "crypto_sign/ed25519/amd64/common/64/init_points4.jinc"
from formosa25519 require "crypto_sign/ed25519/amd64/common/load_ptr4.jinc"

require "ge25519_base_slide_multiples.jinc"
require "ge25519_nielsadd_p1p1.jinc"
require "ge25519_nielssub_p1p1.jinc"
require "ge25519_pnielsadd_p1p1.jinc"
require "ge25519_pnielssub_p1p1.jinc"
require "ge25519_dbl_p1p1.jinc"
require "ge25519_p1p1_to_p2.jinc"
require "ge25519_p1p1_to_p3.jinc"
require "sc25519_slide.jinc"

require "mul4.jinc"
require "mul5.jinc"

param int S1_SWINDOWSIZE = 5;
param int PRE1_SIZE = 8;
param int S2_SWINDOWSIZE = 7;

inline fn ge25519_double_scalarmult_vartime(stack u64[12] r, stack u64[16] p1, reg u64 n0 n1 n2 n3 m0 m1 m2 m3, #msf reg u64 msf) -> stack u64[12], #msf reg u64
{
	reg u8 slide_8;
	reg u64 j k slide_64 windowsize;
	#mmx reg u64 ks;
	reg u64[4] m n;
	reg u64[4] ysubx xaddy t2d;
	reg u64[5] xaddy5 _xaddy5 ysubx5;
	reg u256 v_256;
	stack u8[256] n_slide m_slide;
	stack u64[4] ms;
	stack u64[5] xaddy5s;
	stack u64[12] q_niels;
	stack u64[16] q_pniels t rs;
	stack u64[18] d1 p3;
	stack u64[19] p3_19;
	stack u64[PRE1_SIZE*16] pre1;
	reg ptr u8[256] np mp;
	reg ptr u64[12] pre1p_12 r12;
	reg ptr u64[16] pre1p;
	reg ptr u64[384] pre2p;
	reg ptr u64[4] ptr_arg;
	reg bool cond;
	inline int i offs;
	#mmx reg u64 msf_s;
	
	
	n[0] = n0;
	n[1] = n1;
	n[2] = n2;
	n[3] = n3;
	m[0] = m0;
	m[1] = m1;
	m[2] = m2;
	m[3] = m3;
	
	pre2p = pre2;
	
	// Compute sliding windows
	ms = #copy(m);
	windowsize = S1_SWINDOWSIZE;
	np = n_slide;
	#update_after_call
	np, msf = sc25519_slide(n, np, windowsize, msf);
	n_slide = np;
	
	m = #copy(ms);
	windowsize = S2_SWINDOWSIZE;
	mp = m_slide;
	#update_after_call
	mp, msf = sc25519_slide(m, mp, windowsize, msf);
	m_slide = mp;
	
	msf_s = #mov_msf(msf);
	
	// Precompute multiples
	// Initialize
	for i=0 to 4{
		v_256 = p1[u256 i];
		pre1[u256 i] = v_256;
	}
	
	pre1p_12 = pre1[0:12];
	t, pre1p_12 = ge25519_dbl_p1p1_prep(pre1p_12);
	pre1[0:12] = pre1p_12;
	d1 = ge25519_p1p1_to_p3(t);
	
	ptr_arg = pre1[12:4];
	t2d = __load_ptr4(ptr_arg);
	pre1[12:4] = __mul4_ssr(ec2d, t2d);
	
	// Multiples
	for i=0 to PRE1_SIZE-1{
		offs = 16*(i+1);
		
		pre1p = pre1[16*i:16];
		t = ge25519_pnielsadd_p1p1(d1, pre1p);
		pre1[offs+8:4]  = __mul4_sss(t[4:4], t[12:4]);
		pre1[offs+12:4] = __mul4_sss(t[0:4], t[8:4]);
		xaddy5s = __mul5_sss(t[4:4], t[8:4]);
		ysubx5  = __mul5_rss(t[0:4], t[12:4]);
		
		xaddy5 = #copy(xaddy5s);
		_xaddy5 = #copy(xaddy5);
		
		ysubx = __sub5_rrr(xaddy5, ysubx5);
		pre1[offs:4] = #copy(ysubx);
		xaddy = __add5_rrr(_xaddy5, ysubx5);
		pre1[offs + 4:4] = #copy(xaddy);
		pre1[offs + 12:4] = __mul4_sss(ec2d, pre1[offs + 12:4]);
	}
	
	
	// Computation
	rs = __init_point_inf();
	msf = #mov_msf(msf_s);
	
	j = 255;
	k = -1;
	
	
	while{cond = (j >=s 0);}(cond){
		msf = #update_msf(cond, msf);
		slide_8 = n_slide[(int)j];
		slide_8 |= m_slide[(int)j];
		
		slide_8 = -slide_8;
		slide_64 = (64s)slide_8;
		
		k = j;
		
		j ^= slide_64;
		j -= 1;
		j = #protect(j, msf);
	}
	msf = #update_msf(!cond, msf);
	
	while{cond=(k >=s 0);}(cond){
		msf = #update_msf(cond, msf);
		ks = k;
		
		r12 = rs[0:12];
		t = ge25519_dbl_p1p1(r12); // Correct value here
		
		k = ks;
		slide_8 = n_slide[(int)k];
		slide_8 = #protect_8(slide_8, msf);
		cond = (slide_8 >s 0);
		if(cond){
			msf = #update_msf(cond, msf);
			msf_s = #mov_msf(msf);
			slide_8 = slide_8 >> 1;
			slide_64 = (64u)slide_8;
			_,_,_,_,_,slide_64 = #IMULri ( slide_64, 16 );
			
			for i = 0 to 16{
				q_pniels[i] = pre1[(int)slide_64 + i];
			}
			
			p3 = ge25519_p1p1_to_p3(t);
			t = ge25519_pnielsadd_p1p1(p3, q_pniels);
			msf = #mov_msf(msf_s);
		}
		else{
			msf = #update_msf(!cond, msf);
			cond = (slide_8 <s 0);
			if(cond){
				msf = #update_msf(cond, msf);
				msf_s = #mov_msf(msf);
				p3 = ge25519_p1p1_to_p3(t);
				
				slide_8 = -slide_8;
				slide_8 = slide_8 >> 1;
				slide_64 = (64u)slide_8;
				_,_,_,_,_,slide_64 = #IMULri ( slide_64, 16 );

				for i = 0 to 16{
					q_pniels[i] = pre1[(int)slide_64 + i];
				}
				t = ge25519_pnielssub_p1p1(p3, q_pniels);
				msf = #mov_msf(msf_s);
			}
			else{
				msf = #update_msf(!cond, msf);
			}
		}
		
		k = ks;
		slide_8 = m_slide[(int)k];
		slide_8 = #protect_8(slide_8, msf);
		cond = (slide_8 >s 0);
		if(cond){
			msf = #update_msf(cond, msf);
			msf_s = #mov_msf(msf);
			slide_8 = slide_8 >> 1;
			slide_64 = (64u)slide_8;
			_,_,_,_,_,slide_64 = #IMULri ( slide_64, 12 );
			
			pre2p = pre2;
			for i = 0 to 12{
				q_niels[i] = pre2p[(int)slide_64 + i];
			}
			
			p3_19 = ge25519_p1p1_to_p3_19(t);
			t = ge25519_nielsadd_p1p1(p3_19, q_niels);
			msf = #mov_msf(msf_s);
		}
		else{
			msf = #update_msf(!cond, msf);
			cond = (slide_8 <s 0);
			if(cond){
				msf = #update_msf(cond, msf);
				msf_s = #mov_msf(msf);
				slide_8 = -slide_8;
				slide_8 = slide_8 >> 1;
				slide_64 = (64u)slide_8;
				_,_,_,_,_,slide_64 = #IMULri ( slide_64, 12 );
				
				pre2p = pre2;
				for i = 0 to 12{
					q_niels[i] = pre2p[(int)slide_64 + i];
				}
				p3_19 = ge25519_p1p1_to_p3_19(t);
				t = ge25519_nielssub_p1p1(p3_19, q_niels);
				msf = #mov_msf(msf_s);
			}
			else{
				msf = #update_msf(!cond, msf);
			}
		}
		
		msf_s = #mov_msf(msf);
		rs[0:12] = ge25519_p1p1_to_p2(t);
		msf = #mov_msf(msf_s);
		
		k = ks;
		k -= 1;
	}
	msf = #update_msf(!cond, msf);
	
	for i=0 to 3
	{
		v_256 = #VMOVDQA_256(rs[u256 i]);
		r[u256 i] = #VMOVDQA_256(v_256);
	}
	
	return r, msf;
}

inline fn __ge25519_double_scalarmult_vartime(reg ptr u64[12] r, reg ptr u64[16] p1, reg u64[4] n m, #msf reg u64 msf) -> reg u64[12], #msf reg u64
{
	reg u64 n0 n1 n2 n3 m0 m1 m2 m3;
	
	m0 = m[0];
	m1 = m[1];
	m2 = m[2];
	m3 = m[3];
	n0 = n[0];
	n1 = n[1];
	n2 = n[2];
	n3 = n[3];
	
	r, msf = ge25519_double_scalarmult_vartime(r, p1, n0, n1, n2, n3, m0, m1, m2, m3, msf);
	
	return r, msf;
}
