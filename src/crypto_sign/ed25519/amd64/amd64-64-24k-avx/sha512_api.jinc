/*
State:
0-15: Buffered message (128 bytes)
16-23: Internal SHA state (64 bytes)
24: Input Length (1 byte)
*/

require "sha512.jinc"
require "sha512_globals.jinc"

u256[4] _0_4 = {
	(4u64)[0x0, 0x0, 0x0, 0x0],
	(4u64)[0x1, 0x1, 0x1, 0x1],
	(4u64)[0x2, 0x2, 0x2, 0x2],
	(4u64)[0x3, 0x3, 0x3, 0x3]
};
u256 _15x32 = (4u64)[0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f];
u256 _16x3 = (4u64)[0xff, 0x10, 0x10, 0x10];
u256 _m1x32 = (4u64)[0xffffffffffffffff, 0xffffffffffffffff, 0xffffffffffffffff, 0xffffffffffffffff];
u256 _0_16x2 = (4u64)[0x0f0e0d0c0b0a0908, 0x0706050403020100, 0x0f0e0d0c0b0a0908, 0x0706050403020100];

inline fn shift_bytes(reg u256[4] m_buf, reg u256 block_offs_256 inv_lane_256 lane_256 mask_256 shift_256)->reg u256[8]
{
  reg u256 cond_256 v_256;
  reg u256[5] m_shuf_buf;
  reg u256[8] m_shift;
  inline int i k;

  m_shuf_buf[4] = #set0_256();
  for k=3 downto -1
  {
    m_shuf_buf[k]	= #VPSHUFB_256(m_buf[k], shift_256);	
    v_256 = (256u)#VEXTRACTI128(m_shuf_buf[k], 1);
    v_256 = #VPANDN_256(mask_256, v_256);
    m_shuf_buf[k+1] |= v_256;
    v_256 = #VPERM2I128(m_shuf_buf[k], m_shuf_buf[k], 0x0f);
    m_shuf_buf[k] = #BLENDV_32u8(v_256, m_shuf_buf[k], mask_256);
  }

  v_256 = #VPERM2I128(m_shuf_buf[4], m_shuf_buf[4], 0x01);
  m_shift[4] = #BLENDV_32u8(m_shuf_buf[4], v_256, inv_lane_256);

  for k=3 downto -1
  {
    v_256 = #VPERM2I128(m_shuf_buf[k], m_shuf_buf[k], 0x01);
    m_shift[k] = #BLENDV_32u8(m_shuf_buf[k], v_256, inv_lane_256);
    m_shift[k+1] = #BLENDV_32u8(m_shift[k+1], v_256, lane_256);
  }

  m_shift[5] = #set0_256();
  m_shift[6] = #set0_256();
  m_shift[7] = #set0_256();

  v_256 = #set0_256();
  m_shift[0] = #BLENDV_32u8(m_shift[0], v_256, lane_256);
  for k=1 to 4
  {
    cond_256 = #VPCMPEQ_4u64(block_offs_256, _0_4[k]);
    m_shift[4+k]	= #BLENDV_32u8(m_shift[4+k], m_shift[4], cond_256);
    m_shift[3+k]	= #BLENDV_32u8(m_shift[3+k], m_shift[3], cond_256);
    m_shift[2+k] = #BLENDV_32u8(m_shift[2+k], m_shift[2], cond_256);
    m_shift[1+k] = #BLENDV_32u8(m_shift[1+k], m_shift[1], cond_256);
    m_shift[k] 	 = #BLENDV_32u8(m_shift[k], m_shift[0], cond_256);
    
    for i=0 to k
    {
      m_shift[i] 	= #BLENDV_32u8(m_shift[i], v_256, cond_256);
    }
  }

  return m_shift;
}

inline fn shift_bytes_align(reg u256[4] m_buf, reg u256 block_offs_256)->reg u256[7]
{
	reg u256 cond_256;
	reg u256[7] m_shift;
	inline int k;
	
	for k=0 to 7
	{
		m_shift[k] = #set0_256();
	}
	
	for k=0 to 4
	{
		cond_256 = #VPCMPEQ_4u64(block_offs_256, _0_4[k]);
		m_shift[3+k]	= #BLENDV_32u8(m_shift[3+k], m_buf[3], cond_256);
		m_shift[2+k] 	= #BLENDV_32u8(m_shift[2+k], m_buf[2], cond_256);
		m_shift[1+k] 	= #BLENDV_32u8(m_shift[1+k], m_buf[1], cond_256);
		m_shift[k] 		= #BLENDV_32u8(m_shift[k], m_buf[0], cond_256);
	}
	
	return m_shift;
}

inline fn sha512_init(stack u64[25] pstate) -> stack u64[25]
{
  inline int i;
  reg ptr u64[8] Hp;
  reg u256 z v_256;

  Hp = SHA512_H;
  z = #set0_256();
  
  for i=0 to 4
  { pstate[:u256 i] = z; }
  
  for i=0 to 2
  { v_256 = Hp[:u256 i];
    pstate[:u256 (4 + i)] = v_256; }
  
  pstate[24] = 0;
  
  return pstate;
}

/*
fn sha512_update_1(reg ptr u64[25] pstate, reg u8 m, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 mod_len temp;
	reg bool cond;
	
	// Update len
	temp = pstate[24];
	mod_len = temp;
	temp += 1;
	pstate[24] = temp;
	
	mod_len = mod_len & 0x7f;
	
	pstate[u8 (int)mod_len] = m;
	
	cond = mod_len == 127;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	return pstate, msf;
}

fn sha512_update_8(reg ptr u64[25] pstate, reg u64 m, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 limb_1_size mod_len;
	reg u64 aux mask temp;
	reg u64 c_block;
	stack u64 m_s;
	reg bool cond;
	
	// Update len
	temp = pstate[24];
	mod_len = temp;
	temp += 8;
	pstate[24] = temp;
	
	mod_len = mod_len & 0x7f;
	
	limb_1_size = 0x7;
	limb_1_size = limb_1_size & mod_len;
	limb_1_size = limb_1_size << 3;
	limb_1_size = limb_1_size;
	c_block = mod_len >> 3;
	
	mask = 0x0000000000000001;
	_, _, _, _, _, mask = #SHL(mask, limb_1_size);
	mask -= 1;
	
	_, _, m = #ROL(m, limb_1_size);
	temp = m;
	m = m & mask;
	mask = !mask;
	temp = temp & mask;
	
	aux = pstate[(int) c_block];
	aux = aux | temp;
	pstate[(int) c_block] = aux;
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	m_s = m;
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m = m_s;
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m;
	
	return pstate, msf;
}

fn __sha512_update_32(reg ptr u64[25] pstate, reg u64 m0 m1 m2 m3, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 c_block limb_1_size mod_len;
	reg u64 aux mask temp;
	reg u64[4] m;
	stack u64[4] m_s;
	reg bool cond;
	
	m[0] = m0;
	m[1] = m1;
	m[2] = m2;
	m[3] = m3;
	
	
	// Update len	
	temp = pstate[24];
	mod_len = temp;
	temp += 32;
	pstate[24] = temp;
	
	mod_len = mod_len & 0x7f;
	
	limb_1_size = 0x7;
	limb_1_size = limb_1_size & mod_len;
	limb_1_size = limb_1_size << 3;
	limb_1_size = limb_1_size;
	c_block = mod_len >> 3;
	
	mask = 0x0000000000000001;
	_, _, _, _, _, mask = #SHL(mask, limb_1_size);
	mask -= 1;
	
	_, _, m[0] = #ROL(m[0], limb_1_size);
	temp = m[0];
	m[0] = m[0] & mask;
	mask = !mask;
	temp = temp & mask;
	
	aux = pstate[(int) c_block];
	aux = aux | temp;
	pstate[(int) c_block] = aux;
	
	_, _, m[1] = #ROL(m[1], limb_1_size);
	temp = m[1];
	temp = temp & mask;
	mask = !mask;
	m[1] = m[1] & mask;
	
	m[0] = m[0] | temp;
	
	_, _, m[2] = #ROL(m[2], limb_1_size);
	temp = m[2];
	m[2] = m[2] & mask;
	mask = !mask;
	temp = temp & mask;
	m[1] = m[1] | temp;
	
	_, _, m[3] = #ROL(m[3], limb_1_size);
	temp = m[3];
	temp = temp & mask;
	mask = !mask;
	m[3] = m[3] & mask;
	m[2] = m[2] | temp;
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	m_s = #copy(m);
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[0] = m_s[0];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[0];
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[1] = m_s[1];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[1];
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[2] = m_s[2];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[2];
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[3] = m_s[3];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[3];

	return pstate, msf;
}

inline fn sha512_update_32(reg ptr u64[25] pstate, reg u64[4] m, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 m0 m1 m2 m3;
	
	m0 = m[0];
	m1 = m[1];
	m2 = m[2];
	m3 = m[3];
	#update_after_call
	pstate, msf = __sha512_update_32(pstate, m0, m1, m2, m3, msf);
	
	return pstate, msf;
}*/

#[sct="{ ptr: public, val: { n: secret, s:secret } } * public * secret * msf -> { ptr: public, val: { n: secret, s:secret } } * msf"]
fn sha512_update_ext(reg ptr u64[25] pstate, reg u64 m, reg u64 inlen, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
  reg u64 iterations mod_len total_len;
  #spill_to_mmx reg u64 j;
  reg u64[8] H_r;
  reg u256 block_offs_256 buf_cpy_2 cnt_256 cond_256 m1 modlen_256 v_256;
  reg u256[4] m_buf dig_buf;
  reg u256[3] m_shift;
  reg ptr u64[80] Kp;
  stack ptr u64[25] pstate_s;
  reg bool cond;
  inline int k;
  #mmx reg u64 msf_s;

  m = #protect(m, msf);

  m_buf[0] = [:u256 m];
  m_buf[1] = [:u256 (m + 32)];
  m_buf[2] = [:u256 (m + 64)];
  m_buf[3] = [:u256 (m + 96)];

  m_shift[0] = #VMOVDQA_256(pstate[:u256 0]);
  m_shift[1] = #VMOVDQA_256(pstate[:u256 1]);
  m_shift[2] = #VMOVDQA_256(pstate[:u256 2]);

  for k=0 to 8
  { H_r[k] = pstate[16+k]; }

  inlen = inlen;
  // Update len
  total_len = pstate[24];
  mod_len = total_len;
  total_len = total_len + inlen;
  pstate[24] = total_len;

  mod_len = mod_len & 0x7f;	
  modlen_256 = (256u)#VMOV_64(mod_len);
  modlen_256 = #VPBROADCAST_4u64(modlen_256);

  block_offs_256 = #VPSRL_4u64(modlen_256, 5);
	  
  iterations = mod_len + inlen;
  iterations = iterations >> 7;

  #declassify j = iterations;
  j = #protect(j, msf);

  pstate_s = pstate;
  Kp = SHA512_K;

  while {cond = (j != 0);}(cond) {
    msf = #update_msf(cond, msf);
    msf_s = #mov_msf(msf);
    
    m += 128;
    
    cnt_256 = #set0_256();
    m1 = #VPCMPEQ_4u64(block_offs_256, block_offs_256);
    cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
    dig_buf[0] = #BLENDV_32u8(m_shift[0], m_buf[0], cond_256);
    dig_buf[1] = #VPAND_256(m_buf[1], cond_256);
    dig_buf[2] = #VPAND_256(m_buf[2], cond_256);
    dig_buf[3] = #VPAND_256(m_buf[3], cond_256);
    
    cnt_256 = #VPSUB_4u64(cnt_256, m1);
    cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
    v_256 = #VPAND_256(m_buf[0], cond_256);
    dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
    v_256 = #VPAND_256(m_buf[1], cond_256);
    dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
    v_256 = #VPAND_256(m_buf[2], cond_256);
    dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
    m_shift[0] = #VPAND_256(m_buf[3], cond_256);
    
    cnt_256 = #VPSUB_4u64(cnt_256, m1);
    cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
    v_256 = #VPAND_256(m_shift[1], cond_256);
    dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
    buf_cpy_2 = #VPAND_256(m_buf[0], cond_256);
    v_256 = #VPAND_256(m_buf[1], cond_256);
    dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
    v_256 = #VPAND_256(m_buf[2], cond_256);
    m_shift[0] = #VPOR_256(m_shift[0], v_256);
    m_shift[1] = #VPAND_256(m_buf[3], cond_256);
    
    cnt_256 = #VPSUB_4u64(cnt_256, m1);
    cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
    v_256 = #VPAND_256(m_shift[1], cond_256);
    dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
    v_256 = #VPAND_256(m_shift[2], cond_256);
    buf_cpy_2 = #VPOR_256(buf_cpy_2, v_256);
    v_256 = #VPAND_256(m_buf[0], cond_256);
    dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
    v_256 = #VPAND_256(m_buf[1], cond_256);
    m_shift[0] = #VPOR_256(m_shift[0], v_256);
    v_256 = #VPAND_256(m_buf[2], cond_256);
    m_shift[1] = #VPOR_256(m_shift[1], v_256);
    m_shift[2] = #VPAND_256(m_buf[3], cond_256);
    
    dig_buf[2] = #VPOR_256(dig_buf[2], buf_cpy_2);
    () = #spill(j);
    m_buf[0] = [:u256 m];
    
    H_r = __inner_sha(H_r, Kp, dig_buf);
    msf = #mov_msf(msf_s);
    () = #unspill(j);
    m_buf[1] = [:u256 (m + 32)];
    m_buf[2] = [:u256 (m + 64)];
    m_buf[3] = [:u256 (m + 96)];
    
    j -= 1;
  }
  msf = #update_msf(!cond, msf);

  pstate = pstate_s;


  for k=0 to 8
  {
    pstate[:u64 (16+k)] = H_r[k];
  }

  cnt_256 = #set0_256();
  m1 = #VPCMPEQ_4u64(block_offs_256, block_offs_256);
  cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
  dig_buf[0] = #BLENDV_32u8(m_shift[0], m_buf[0], cond_256);
  dig_buf[1] = #VPAND_256(m_buf[1], cond_256);
  dig_buf[2] = #VPAND_256(m_buf[2], cond_256);
  dig_buf[3] = #VPAND_256(m_buf[3], cond_256);

  cnt_256 = #VPSUB_4u64(cnt_256, m1);
  cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
  v_256 = #VPAND_256(m_buf[0], cond_256);
  dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
  v_256 = #VPAND_256(m_buf[1], cond_256);
  dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
  v_256 = #VPAND_256(m_buf[2], cond_256);
  dig_buf[3] = #VPOR_256(dig_buf[3], v_256);

  cnt_256 = #VPSUB_4u64(cnt_256, m1);
  cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
  v_256 = #VPAND_256(m_shift[1], cond_256);
  dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
  v_256 = #VPAND_256(m_buf[0], cond_256);
  dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
  v_256 = #VPAND_256(m_buf[1], cond_256);
  dig_buf[3] = #VPOR_256(dig_buf[3], v_256);

  cnt_256 = #VPSUB_4u64(cnt_256, m1);
  cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
  v_256 = #VPAND_256(m_shift[1], cond_256);
  dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
  v_256 = #VPAND_256(m_shift[2], cond_256);
  dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
  v_256 = #VPAND_256(m_buf[0], cond_256);
  dig_buf[3] = #VPOR_256(dig_buf[3], v_256);

  for k=0 to 4
  {
    pstate[:u256 k] = #VMOVDQA_256(dig_buf[k]);
  }

  return pstate, msf;
}

#[sct="{ ptr: public, val: { n: secret, s:secret } } * { ptr: public, val: { n: secret, s:secret } } * msf -> { ptr: public, val: { n: secret, s:secret } } * msf"]
fn sha512_finalize(reg ptr u64[25] pstate, reg ptr u64[8] Hreg, #msf reg u64 msf) -> reg ptr u64[8], #msf reg u64
{
  #secret reg u64 len mod_len nblocks temp;
  reg u256 bswap_256 v_256;	
  #secret stack u8[128] in;
  #secret stack u64[32] sblocks;
  reg ptr u64[32] sblocksp;
  inline int i;
		
  for i = 0 to 4{
    v_256 = #VMOVDQA_256(pstate[:u256 i]);
    in[:u256 i] = #VMOVDQA_256(v_256);
  }

  len = pstate[24];
  temp = len;
  mod_len = len & 0x7f;
  temp = temp << 3;

  sblocks, nblocks, msf = __lastblocks_ref(in, mod_len, temp, msf);
  sblocksp = sblocks;
  
  for i=0 to 2{
    v_256 = #VMOVDQA_256(pstate[:u256 4+i]);
    Hreg[:u256 i] = #VMOVDQA_256(v_256);
  }
  
  Hreg, _, msf = _blocks_1_ref(Hreg, sblocksp, nblocks, msf);
  
  bswap_256 = bswap_indices;
  for i=0 to 2
  { 
    v_256 = #VMOVDQA_256(Hreg[:u256 i]);
    v_256 = #VPSHUFB_256(v_256, bswap_256);
    Hreg[:u256 i] = #VMOVDQA_256(v_256);
  }
 
  return Hreg, msf;
}
