/*
State:
0-15: Buffered message (128 bytes)
16-23: Internal SHA state (64 bytes)
24: Input Length (1 byte)
*/

require "sha512.jinc"
require "sha512_globals.jinc"

u256[4] _0_4 = {
	(4u64)[0x0, 0x0, 0x0, 0x0],
	(4u64)[0x1, 0x1, 0x1, 0x1],
	(4u64)[0x2, 0x2, 0x2, 0x2],
	(4u64)[0x3, 0x3, 0x3, 0x3]
};
u256 _15x32 = (4u64)[0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f];
u256 _16x3 = (4u64)[0xff, 0x10, 0x10, 0x10];
u256 _m1x32 = (4u64)[0xffffffffffffffff, 0xffffffffffffffff, 0xffffffffffffffff, 0xffffffffffffffff];
u256 _0_16x2 = (4u64)[0x0f0e0d0c0b0a0908, 0x0706050403020100, 0x0f0e0d0c0b0a0908, 0x0706050403020100];

inline fn shift_bytes(reg u256[4] m_buf, reg u256 block_offs_256 inv_lane_256 lane_256 mask_256 shift_256)->reg u256[8]
{
	reg u256 cond_256 v_256;
	reg u256[5] m_shuf_buf;
	reg u256[8] m_shift;
	inline int i k;
	
	m_shuf_buf[4] = #set0_256();
	for k=3 downto -1
	{
		m_shuf_buf[k]	= #VPSHUFB_256(m_buf[k], shift_256);	
		v_256 = (256u)#VEXTRACTI128(m_shuf_buf[k], 1);
		v_256 = #VPANDN_256(mask_256, v_256);
		m_shuf_buf[k+1] |= v_256;
		v_256 = #VPERM2I128(m_shuf_buf[k], m_shuf_buf[k], 0x0f);
		m_shuf_buf[k] = #VPBLENDVB_256(v_256, m_shuf_buf[k], mask_256);
	}
	
	v_256 = #VPERM2I128(m_shuf_buf[4], m_shuf_buf[4], 0x01);
	m_shift[4] = #VPBLENDVB_256(m_shuf_buf[4], v_256, inv_lane_256);
	
	for k=3 downto -1
	{
		v_256 = #VPERM2I128(m_shuf_buf[k], m_shuf_buf[k], 0x01);
		m_shift[k] = #VPBLENDVB_256(m_shuf_buf[k], v_256, inv_lane_256);
		m_shift[k+1] = #VPBLENDVB_256(m_shift[k+1], v_256, lane_256);
	}
	
	m_shift[5] = #set0_256();
	m_shift[6] = #set0_256();
	m_shift[7] = #set0_256();
	
	v_256 = #set0_256();
	m_shift[0] = #VPBLENDVB_256(m_shift[0], v_256, lane_256);
	for k=1 to 4
	{
		cond_256 = #VPCMPEQ_4u64(block_offs_256, _0_4[k]);
		m_shift[4+k]	= #VPBLENDVB_256(m_shift[4+k], m_shift[4], cond_256);
		m_shift[3+k]	= #VPBLENDVB_256(m_shift[3+k], m_shift[3], cond_256);
		m_shift[2+k] 	= #VPBLENDVB_256(m_shift[2+k], m_shift[2], cond_256);
		m_shift[1+k] 	= #VPBLENDVB_256(m_shift[1+k], m_shift[1], cond_256);
		m_shift[k] 		= #VPBLENDVB_256(m_shift[k], m_shift[0], cond_256);
		
		for i=0 to k
		{
			m_shift[i] 	= #VPBLENDVB_256(m_shift[i], v_256, cond_256);
		}
	}
	
	return m_shift;
}

inline fn shift_bytes_align(reg u256[4] m_buf, reg u256 block_offs_256)->reg u256[7]
{
	reg u256 cond_256;
	reg u256[7] m_shift;
	inline int k;
	
	for k=0 to 7
	{
		m_shift[k] = #set0_256();
	}
	
	for k=0 to 4
	{
		cond_256 = #VPCMPEQ_4u64(block_offs_256, _0_4[k]);
		m_shift[3+k]	= #VPBLENDVB_256(m_shift[3+k], m_buf[3], cond_256);
		m_shift[2+k] 	= #VPBLENDVB_256(m_shift[2+k], m_buf[2], cond_256);
		m_shift[1+k] 	= #VPBLENDVB_256(m_shift[1+k], m_buf[1], cond_256);
		m_shift[k] 		= #VPBLENDVB_256(m_shift[k], m_buf[0], cond_256);
	}
	
	return m_shift;
}

inline fn sha512_init(stack u64[25] pstate) -> stack u64[25]
{
  inline int i;
  reg ptr u64[8] Hp;
  reg u256 z v_256;

  Hp = SHA512_H;
	
	z = #set0_256();
  for i=0 to 4
  {
  	pstate[u256 i] = z;
  }
  for i=0 to 2
  { v_256 = Hp[u256 i];
    pstate[u256 4 + i] = v_256; }
  
  pstate[24] = 0;
  
  return pstate;
}

/*
fn sha512_update_1(reg ptr u64[25] pstate, reg u8 m, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 mod_len temp;
	reg bool cond;
	
	// Update len
	temp = pstate[24];
	mod_len = temp;
	temp += 1;
	pstate[24] = temp;
	
	mod_len = mod_len & 0x7f;
	
	pstate[u8 (int)mod_len] = m;
	
	cond = mod_len == 127;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	return pstate, msf;
}

fn sha512_update_8(reg ptr u64[25] pstate, reg u64 m, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 limb_1_size mod_len;
	reg u64 aux mask temp;
	reg u64 c_block;
	stack u64 m_s;
	reg bool cond;
	
	// Update len
	temp = pstate[24];
	mod_len = temp;
	temp += 8;
	pstate[24] = temp;
	
	mod_len = mod_len & 0x7f;
	
	limb_1_size = 0x7;
	limb_1_size = limb_1_size & mod_len;
	limb_1_size = limb_1_size << 3;
	limb_1_size = limb_1_size;
	c_block = mod_len >> 3;
	
	mask = 0x0000000000000001;
	_, _, _, _, _, mask = #SHL(mask, limb_1_size);
	mask -= 1;
	
	_, _, m = #ROL(m, limb_1_size);
	temp = m;
	m = m & mask;
	mask = !mask;
	temp = temp & mask;
	
	aux = pstate[(int) c_block];
	aux = aux | temp;
	pstate[(int) c_block] = aux;
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	m_s = m;
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m = m_s;
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m;
	
	return pstate, msf;
}

fn __sha512_update_32(reg ptr u64[25] pstate, reg u64 m0 m1 m2 m3, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 c_block limb_1_size mod_len;
	reg u64 aux mask temp;
	reg u64[4] m;
	stack u64[4] m_s;
	reg bool cond;
	
	m[0] = m0;
	m[1] = m1;
	m[2] = m2;
	m[3] = m3;
	
	
	// Update len	
	temp = pstate[24];
	mod_len = temp;
	temp += 32;
	pstate[24] = temp;
	
	mod_len = mod_len & 0x7f;
	
	limb_1_size = 0x7;
	limb_1_size = limb_1_size & mod_len;
	limb_1_size = limb_1_size << 3;
	limb_1_size = limb_1_size;
	c_block = mod_len >> 3;
	
	mask = 0x0000000000000001;
	_, _, _, _, _, mask = #SHL(mask, limb_1_size);
	mask -= 1;
	
	_, _, m[0] = #ROL(m[0], limb_1_size);
	temp = m[0];
	m[0] = m[0] & mask;
	mask = !mask;
	temp = temp & mask;
	
	aux = pstate[(int) c_block];
	aux = aux | temp;
	pstate[(int) c_block] = aux;
	
	_, _, m[1] = #ROL(m[1], limb_1_size);
	temp = m[1];
	temp = temp & mask;
	mask = !mask;
	m[1] = m[1] & mask;
	
	m[0] = m[0] | temp;
	
	_, _, m[2] = #ROL(m[2], limb_1_size);
	temp = m[2];
	m[2] = m[2] & mask;
	mask = !mask;
	temp = temp & mask;
	m[1] = m[1] | temp;
	
	_, _, m[3] = #ROL(m[3], limb_1_size);
	temp = m[3];
	temp = temp & mask;
	mask = !mask;
	m[3] = m[3] & mask;
	m[2] = m[2] | temp;
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	m_s = #copy(m);
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[0] = m_s[0];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[0];
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[1] = m_s[1];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[1];
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[2] = m_s[2];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[2];
	
	c_block += 1;
	c_block = c_block & 0xf;
	() = #spill(c_block);
	
	cond = c_block == 0;
	if (cond) {
		msf = #update_msf(cond, msf);
		#update_after_call
		pstate, msf = _blocks_0_ref(pstate, msf);
	}
	else {
		msf = #update_msf(!cond, msf);
	}
	
	m[3] = m_s[3];
	
	() = #unspill(c_block);
	c_block = #protect(c_block, msf);
	pstate[(int) c_block] = m[3];

	return pstate, msf;
}

inline fn sha512_update_32(reg ptr u64[25] pstate, reg u64[4] m, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 m0 m1 m2 m3;
	
	m0 = m[0];
	m1 = m[1];
	m2 = m[2];
	m3 = m[3];
	#update_after_call
	pstate, msf = __sha512_update_32(pstate, m0, m1, m2, m3, msf);
	
	return pstate, msf;
}*/

#[sct="{ ptr: public, val: { n: d, s:secret } } * public * secret * msf -> { ptr: public, val: { n: d, s:secret } } * msf"]
fn sha512_update_ext(reg ptr u64[25] pstate, reg u64 m, reg u64 inlen, #msf reg u64 msf) -> reg ptr u64[25], #msf reg u64
{
	reg u64 iterations mod_len total_len;
	#public #spill_to_mmx reg u64 j;
	reg u64[8] H_r;
	reg u256 block_offs_256 buf_cpy_2 cnt_256 cond_256 m1 modlen_256 v_256;
	reg u256[4] m_buf dig_buf;
	reg u256[3] m_shift;
	reg ptr u64[80] Kp;
	stack ptr u64[25] pstate_s;
	reg bool cond;
	inline int k;
	#mmx reg u64 msf_s;
	
	m = #protect(m, msf);
	
	m_buf[0] = (u256)[m];
	m_buf[1] = (u256)[m + 32];
	m_buf[2] = (u256)[m + 64];
	m_buf[3] = (u256)[m + 96];
	
	m_shift[0] = #VMOVDQA_256(pstate[u256 0]);
  m_shift[1] = #VMOVDQA_256(pstate[u256 1]);
  m_shift[2] = #VMOVDQA_256(pstate[u256 2]);
  
  for k=0 to 8
  {
  	H_r[k] = pstate[u64 16+k];
  }
  
	inlen = inlen;
	// Update len
	total_len = pstate[24];
	mod_len = total_len;
	total_len = total_len + inlen;
	pstate[24] = total_len;
	
	mod_len = mod_len & 0x7f;	
	modlen_256 = (256u)#VMOV_64(mod_len);
	modlen_256 = #VPBROADCAST_4u64(modlen_256);
	
	block_offs_256 = #VPSRL_4u64(modlen_256, 5);
		
	iterations = mod_len + inlen;
	iterations = iterations >> 7;
	
	modlen_256 = #VPBROADCAST_32u8(modlen_256);
	
	#declassify j = iterations;
	j = #protect(j, msf);
  
  pstate_s = pstate;
  Kp = SHA512_K;
  
	while {cond = (j != 0);}(cond) {
		msf = #update_msf(cond, msf);
		msf_s = #mov_msf(msf);
		
		m += 128;
		
		cnt_256 = #set0_256();
		m1 = #VPCMPEQ_4u64(block_offs_256, block_offs_256);
		cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
		dig_buf[0] = #VPBLENDVB_256(m_shift[0], m_buf[0], cond_256);
		dig_buf[1] = #VPAND_256(m_buf[1], cond_256);
		dig_buf[2] = #VPAND_256(m_buf[2], cond_256);
		dig_buf[3] = #VPAND_256(m_buf[3], cond_256);
		
		cnt_256 = #VPSUB_4u64(cnt_256, m1);
		cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
		v_256 = #VPAND_256(m_buf[0], cond_256);
		dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
		v_256 = #VPAND_256(m_buf[1], cond_256);
		dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
		v_256 = #VPAND_256(m_buf[2], cond_256);
		dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
		m_shift[0] = #VPAND_256(m_buf[3], cond_256);
		
		cnt_256 = #VPSUB_4u64(cnt_256, m1);
		cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
		v_256 = #VPAND_256(m_shift[1], cond_256);
		dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
		buf_cpy_2 = #VPAND_256(m_buf[0], cond_256);
		v_256 = #VPAND_256(m_buf[1], cond_256);
		dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
		v_256 = #VPAND_256(m_buf[2], cond_256);
		m_shift[0] = #VPOR_256(m_shift[0], v_256);
		m_shift[1] = #VPAND_256(m_buf[3], cond_256);
		
		cnt_256 = #VPSUB_4u64(cnt_256, m1);
		cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
		v_256 = #VPAND_256(m_shift[1], cond_256);
		dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
		v_256 = #VPAND_256(m_shift[2], cond_256);
		buf_cpy_2 = #VPOR_256(buf_cpy_2, v_256);
		v_256 = #VPAND_256(m_buf[0], cond_256);
		dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
		v_256 = #VPAND_256(m_buf[1], cond_256);
		m_shift[0] = #VPOR_256(m_shift[0], v_256);
		v_256 = #VPAND_256(m_buf[2], cond_256);
		m_shift[1] = #VPOR_256(m_shift[1], v_256);
		m_shift[2] = #VPAND_256(m_buf[3], cond_256);
		
		dig_buf[2] = #VPOR_256(dig_buf[2], buf_cpy_2);
		() = #spill(j);
		m_buf[0] = (u256)[m];
		
		H_r = __inner_sha(H_r, Kp, dig_buf);
		msf = #mov_msf(msf_s);
		() = #unspill(j);
		m_buf[1] = (u256)[m + 32];
		m_buf[2] = (u256)[m + 64];
		m_buf[3] = (u256)[m + 96];
		
		j -= 1;
	}
	msf = #update_msf(!cond, msf);
	
	pstate = pstate_s;
  
  
  for k=0 to 8
  {
  	pstate[u64 16+k] = H_r[k];
  }
	
	cnt_256 = #set0_256();
	m1 = #VPCMPEQ_4u64(block_offs_256, block_offs_256);
	cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
	dig_buf[0] = #VPBLENDVB_256(m_shift[0], m_buf[0], cond_256);
	dig_buf[1] = #VPAND_256(m_buf[1], cond_256);
	dig_buf[2] = #VPAND_256(m_buf[2], cond_256);
	dig_buf[3] = #VPAND_256(m_buf[3], cond_256);
	
	cnt_256 = #VPSUB_4u64(cnt_256, m1);
	cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
	v_256 = #VPAND_256(m_buf[0], cond_256);
	dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
	v_256 = #VPAND_256(m_buf[1], cond_256);
	dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
	v_256 = #VPAND_256(m_buf[2], cond_256);
	dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
	
	cnt_256 = #VPSUB_4u64(cnt_256, m1);
	cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
	v_256 = #VPAND_256(m_shift[1], cond_256);
	dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
	v_256 = #VPAND_256(m_buf[0], cond_256);
	dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
	v_256 = #VPAND_256(m_buf[1], cond_256);
	dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
	
	cnt_256 = #VPSUB_4u64(cnt_256, m1);
	cond_256 = #VPCMPEQ_4u64(block_offs_256, cnt_256);
	v_256 = #VPAND_256(m_shift[1], cond_256);
	dig_buf[1] = #VPOR_256(dig_buf[1], v_256);
	v_256 = #VPAND_256(m_shift[2], cond_256);
	dig_buf[2] = #VPOR_256(dig_buf[2], v_256);
	v_256 = #VPAND_256(m_buf[0], cond_256);
	dig_buf[3] = #VPOR_256(dig_buf[3], v_256);
	
	for k=0 to 4
	{
		pstate[u256 k] = #VMOVDQA_256(dig_buf[k]);
	}

	return pstate, msf;
}

#[sct="{ ptr: public, val: { n: d, s:secret } } * { ptr: public, val: { n: d, s:secret } } * msf -> { ptr: public, val: { n: d, s:secret } } * msf"]
fn sha512_finalize(reg ptr u64[25] pstate, reg ptr u64[8] Hreg, #msf reg u64 msf) -> reg ptr u64[8], #msf reg u64
{
	#secret reg u64 len mod_len nblocks temp;
	reg u256 bswap_256 v_256;	
	#secret stack u8[128] in;
	#secret stack u64[32] sblocks;
  reg ptr u64[32] sblocksp;
  inline int i;
	
	
	for i = 0 to 4{
		v_256 = #VMOVDQA_256(pstate[u256 i]);
		in[u256 i] = #VMOVDQA_256(v_256);
  }
	
	len = pstate[24];
	temp = len;
	mod_len = len & 0x7f;
	temp = temp << 3;
	
	sblocks, nblocks, msf = __lastblocks_ref(in, mod_len, temp, msf);
  sblocksp = sblocks;
  
  for i=0 to 2{
		v_256 = #VMOVDQA_256(pstate[u256 4+i]);
		Hreg[u256 i] = #VMOVDQA_256(v_256);
  }
  
  Hreg, _, msf = _blocks_1_ref(Hreg, sblocksp, nblocks, msf);
  
  bswap_256 = bswap_indices;
  for i=0 to 2
  { 
  	v_256 = #VMOVDQA_256(Hreg[u256 i]);
  	v_256 = #VPSHUFB_256(v_256, bswap_256);
    Hreg[u256 i] = #VMOVDQA_256(v_256);
  }
 
  return Hreg, msf;
}
